{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a31557e4",
   "metadata": {},
   "source": [
    "# CS172 Assignment 1: CAPTCHAs\n",
    "Shanghaitech University 2025 Fall Computer Vision I\n",
    "\n",
    "Welcome to the first assignment of CS172. In this homework, we aim to build and train a neural network model to recognize CAPTCHAs consisting only of Arabic numerals (0-9). The primary focus is on generating synthetic verification code data, preprocessing it, and building a convolutional neural network (CNN) for character recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb446811",
   "metadata": {},
   "source": [
    "## Part 0: Setup\n",
    "Before running the notebook, ensure that your environment is properly set up. Install the necessary dependencies, which include `torch`, `torchvision`, `PIL`, and `matplotlib`. These can be installed via conda and pip using the command:\n",
    "```\n",
    "conda create -n 172a1 python=3.8\n",
    "conda activate 172a1\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "Then you can run the following code to load the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68175ca-fa3d-4a93-be0b-85ca5ac7942b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, os\n",
    "from cs172 import utils\n",
    "from cs172 import generate\n",
    "from cs172.datasets import ImageDataset\n",
    "from cs172.networks import get_model\n",
    "from cs172.pipeline import train, test\n",
    "\n",
    "utils.set_seed(172)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c99a70",
   "metadata": {},
   "source": [
    "## Part 1: Data Generation\n",
    "Implement the `generate_verification` function in `cs172/generate.py`, where we generate synthetic images of verification codes. Each image contains a series of Arabic numerals. \n",
    "You can modify parameters such as colors, noise level and noise type.\n",
    "\n",
    "After implementing this function, you can display some data by running the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942264d5-bc7e-4854-952c-62b73d44678a",
   "metadata": {},
   "outputs": [],
   "source": [
    "need_rotate = True # Do not modify this line\n",
    "images = [generate.generate_verification(need_rotate=need_rotate)[0] for _ in range(16)]\n",
    "display(utils.merge_images(images))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035433b4",
   "metadata": {},
   "source": [
    "To generate the training dataset, run the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2626cc-7c8b-4705-bf5d-10787df4651b",
   "metadata": {},
   "outputs": [],
   "source": [
    "need_rotate = True # if you do not want to rotate the image for training, feel free to set it to False\n",
    "image_size = (260, 80)\n",
    "data_num = 100000 # total data num\n",
    "save_folder = 'data' # set data folder\n",
    "\n",
    "# You can comment out the line to avoid duplicate data generation\n",
    "generate.save_certification_data(image_size, data_num, save_folder, need_rotate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f613b25a",
   "metadata": {},
   "source": [
    "Discussion:\n",
    "- What are some real-world challenges you might encounter when working with CAPTCHA images compared to our synthetic ones (e.g., distortions, noise)?\n",
    "    - Real-world captchas often feature strong distortions, curved baselines, complex background textures, close foreground/background colors, character adhesion, varying font sizes and shapes, motion blur, compression artifacts, and dynamic generation strategies. All of these make them much harder to model and segment than our controlled synthetic data.\n",
    "- Would using different fonts or noise levels impact the model's learning ability? How would you test this?\n",
    "    - Fonts and noise determine whether the model can learn generalized features. Their impact can be evaluated by generating datasets with different fonts/noise intensities, using the same training process to compare the performance on the validation set, or conducting grouped ablation experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354d5a4c",
   "metadata": {},
   "source": [
    "## Part 2: Data Preprossing\n",
    "Implement `__len__` and `__getitem__` functions in `cs172/datasets.py` for a customized `Dataset` class.\n",
    "\n",
    "Then you can run the following code to preprocess the generated images to fit the neural network's input and batch the data using Pytorch's `DataLoader`. \n",
    "\n",
    "You can change the `transform` variable which may include steps such as resizing the images, normalizing pixel values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d964cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can try different transform for data augmentation\n",
    "# torchvision.transforms may be useful\n",
    "transform = None\n",
    "\n",
    "save_folder = 'data'\n",
    "dataset = ImageDataset(imgdir=save_folder, transform=transform)\n",
    "\n",
    "print(len(dataset))\n",
    "\n",
    "ratio = 0.8\n",
    "train_size = int(ratio * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, valid_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# you can modify batch_size if 'CUDA OUT OF MEMORY'\n",
    "batch_size = 256\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size,\n",
    "    shuffle=True, num_workers=0)\n",
    "valid_dataloader = torch.utils.data.DataLoader(\n",
    "    valid_dataset, batch_size=batch_size,\n",
    "    shuffle=False, num_workers=0)\n",
    "\n",
    "img, label = next(iter(train_dataloader))\n",
    "print(img[0].min(), img[0].max())\n",
    "print(label[0])\n",
    "img.shape, label.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9516f0",
   "metadata": {},
   "source": [
    "Discussion:\n",
    "- How important is data normalization in neural networks? What could happen if you skip this step?\n",
    "    - Normalization stabilizes feature distributions, balances gradients more effectively, and accelerates training. If normalization is skipped, gradients may explode or vanish, leading to slow convergence, extreme sensitivity to initialization, and even training failure.\n",
    "- What effect does batch size have on model performance and training speed? How would you balance memory usage with batch size selection?\n",
    "    - Small batches provide noisier gradients, which helps with generalization but leads to unstable training. Large batches, on the other hand, result in smoother convergence but require more memory and are prone to getting trapped in sharp minima. Typically, the largest possible batch size is chosen based on memory limits, followed by adjustments to balance stability and speed, often combined with learning rate scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56b9814",
   "metadata": {},
   "source": [
    "## Part 3: Model Architecture\n",
    "Implement ResNet18 in `networks.py`, which contains `SimpleResBlock` and `ResNet18` class. \n",
    "\n",
    "For `SimpleResBlock`, you can follow this structure to implement each layer(weight layer, activation funcyion, residual...)\n",
    "\n",
    "The residual block has two $3 \\times 3$ convolutional layers with the same number of output channels. Each convolutional layer is followed by a batch normalization layer and a ReLU activation function. \n",
    "Then, we skip these two convolution operations and add the input directly before the final ReLU activation function. \n",
    "This kind of design requires that the output of the two convolutional layers has to be of the same shape as the input, so that they can be added together. \n",
    "If we want to change the number of channels, we need to introduce an additional $1 \\times 1$ convolutional layer to transform the input into the desired shape for the addition operation. \n",
    "\n",
    "![ResBlock](assets/resnet-block.svg \"picture is from https://d2l.ai/\")\n",
    "\n",
    "For `ResNet18`, you can follow this structure:\n",
    "\n",
    "The first two layers of ResNet are the same as those of the GoogLeNet we described before: the $7 \\times 7$ convolutional layer with 64 output channels and a stride of 2 is followed by the $3 \\times 3$ max-pooling layer with a stride of 2. The difference is the batch normalization layer added after each convolutional layer in ResNet.\n",
    "\n",
    "ResNet uses four modules made up of residual blocks, each of which uses several residual blocks with the same number of output channels. The number of channels in the first module is the same as the number of input channels. Since a max-pooling layer with a stride of 2 has already been used, it is not necessary to reduce the height and width. In the first residual block for each of the subsequent modules, the number of channels is doubled compared with that of the previous module, and the height and width are halved.\n",
    "\n",
    "Then, we add all the modules to ResNet. Here, two residual blocks are used for each module. Lastly, we add a global average pooling layer, followed by the fully connected layer output.\n",
    "\n",
    "There are four convolutional layers in each module (excluding the $1 \\times 1$ convolutional layer). Together with the first $7 \\times 7$ convolutional layer and the final fully connected layer, there are 18 layers in total. Therefore, this model is commonly known as ResNet-18. \n",
    "\n",
    "![CAPTCHAs](assets/resnet18-90.svg \"picture is from https://d2l.ai/, which is a very useful deep learning book\") \n",
    "\n",
    "Pictures and descriptions come from [Dive into Deep Learning](https://d2l.ai/), which is a very helpful deep learning book.\n",
    "\n",
    "After implementing the networks architecture, you can run the following code to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d540247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change model_name to compare with the ResNet implementation in torchvision\n",
    "model_name = \"myresnet18\"\n",
    "assert model_name in (\"myresnet18\", \"resnet18\", \"resnet34\")\n",
    "model = get_model(model_name)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22cabea",
   "metadata": {},
   "source": [
    "Discussion:\n",
    "- What modifications could you make to the current architecture if the model struggles with overfitting or underfitting?\n",
    "    - When underfitting occurs, you can increase the number of channels, add more residual blocks, switch to stronger activation functions, or expand the input resolution. For overfitting, you can introduce dropout, increase weight decay, apply data augmentation (such as rotation, affine transformation, and color perturbation), reduce the model width, or use label smoothing.\n",
    "- Would you consider using a different type of model (like MLP) for this task? Why or why not?\n",
    "    - MLPs flatten images, which leads to the loss of spatial structure, an enormous number of parameters, and poor translational invariance. Convolutional networks, with their local receptive fields and weight sharing, are more suitable for captcha recognition. Therefore, unless the input has been pre-extracted into feature vectors, it is not recommended to switch to MLPs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0f37b5",
   "metadata": {},
   "source": [
    "## Part 4: Training\n",
    "In`pipeline.py`, implement a training loop for the model. You will fine-tune hyperparameters like learning rate, batch size, and optimizer, while monitoring the model's performance with loss and accuracy metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7505dc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "if os.path.exists(\"model_weights.pth\"):\n",
    "    model_weights = torch.load(\"model_weights.pth\")\n",
    "    model.load_state_dict(model_weights)\n",
    "else:\n",
    "    # you can try different hyperparameters by passing args `lr`, `weight_decay`, `num_epoch`\n",
    "    train(model, device, train_dataloader,lr=1e-4)\n",
    "\n",
    "# save model weights, thus you can try different hyperparameters without retraining\n",
    "torch.save(model.state_dict(), \"model_weights.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cc2438",
   "metadata": {},
   "source": [
    "|   lr   | weight_decay | echo | train_image_accuracy | train_digit_accuracy | valid_image_accuracy | valid_digit_accuracy |\n",
    "|--------|--------------|------|----------------------|----------------------|----------------------|----------------------|\n",
    "| 1e-3   |     0.05     |  10  |       0.45996        |       0.01401        |       0.01232        |      0.00128         |\n",
    "| 1e-3| 0.05|30|0.42848|0.01127|0.62000|0.02999|\n",
    "|1e-4|0.05|10|0.99007|0.97012|0.99163|0.96415|\n",
    "|5e-5|0.005|10|0.99931|0.99125|0.32312|0.00528|\n",
    "\n",
    "\n",
    "First, noticing the low accuracy on the training set, I thought of increasing the number of training epochs, but the result was still unsatisfactory. Then I considered that the excessively large learning rate prevented effective gradient descent. So I reduced the learning rate and achieved good results. After that, I tried to find a smaller learning rate and weaken the regularization constraint, only to find that the performance on the validation set became worse. Finally, I concluded that this was caused by overfitting due to the excessively small learning rate and regularization.\n",
    "\n",
    "\n",
    "Since the data in the test is divided into five parts, we will also split the training data into five equal parts to better align with the test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1075f1",
   "metadata": {},
   "source": [
    "Discussion:\n",
    "- How would you approach adjusting the learning rate if the model's accuracy plateaus early? Could a learning rate schedule improve training?\n",
    "    - If accuracy stagnates early, you can first try reducing the learning rate or switching to schedulers such as cosine annealing, StepLR, or OneCycle. Additionally, you can trigger ReduceLROnPlateau when a plateau is reached or use warm restarts to escape local optima.\n",
    "- Would regularization methods (such as dropout or L2 regularization) be beneficial for this task? When would you apply them?\n",
    "    - L2 regularization, dropout, data augmentation, label smoothing, and other methods can all alleviate overfitting. These regularization techniques should be introduced when the training accuracy is much higher than that of the validation set or when the validation loss rebounds\n",
    "- How would you know when to stop training the model? Should early stopping be implemented, and if so, how would you determine when?\n",
    "    - The decision to stop training should be based on the trend of loss/accuracy on the validation set. You can implement early stopping with a patience value (e.g., stopping training if the validation metric shows no improvement for N consecutive epochs) and save the best weights, thereby avoiding overfitting and ineffective training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08322285",
   "metadata": {},
   "source": [
    "## Part 5: Evaluation\n",
    "After training, evaluate the model's performance on the validation and test datasets using metrics such as digit accuracy and image accuracy. Visualize predictions and errors to understand the model’s strengths and weaknesses.\n",
    "You could add more experiments and disscussion (like loss decay, metric table, hyperparameter ablation study) in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080deaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test on your valid data\n",
    "device = \"cuda\" if torch.cuda.is_available() else 'cpu'\n",
    "metrics = test(model, device, valid_dataloader)\n",
    "for key, value in metrics.items():\n",
    "    print(f\"{key}:\", value.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123fa10f",
   "metadata": {},
   "source": [
    "Experiment with various hyperparameters — including learning rates, batch sizes, network architecture (e.g., number of layers, filter sizes, and activations), loss decay, and metric settings — on your own generated validation set. Analyze the results and report the best-performing configuration here:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ada45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test on sample test data\n",
    "# There are 5 test sets in 'samples'. To receive full points for each test set,\n",
    "# your model must achieve a digit accuracy ≥ 0.95 and an image accuracy ≥ 0.75.\n",
    "folders = sorted(os.listdir('samples'))\n",
    "for folder in folders:\n",
    "    if folder == \"test_alphabet\":\n",
    "        continue \n",
    "\n",
    "    test_dataset = ImageDataset(os.path.join('samples', folder), transform=transform)\n",
    "    test_dataloader = torch.utils.data.DataLoader(\n",
    "        test_dataset, batch_size=batch_size,\n",
    "        shuffle=False, num_workers=0,\n",
    "    )\n",
    "    metrics = test(model, device, test_dataloader)\n",
    "    for key, value in metrics.items():\n",
    "        print(f\"{key} for {folder}:\", value.item())\n",
    "    \n",
    "    model.eval()\n",
    "    images, imgs, labels = zip(*[test_dataset.get_samples(idx) for idx in range(4)])\n",
    "    labels = [label.reshape(5, 10).argmax(axis=-1).tolist() for label in labels]\n",
    "    with torch.no_grad():\n",
    "        preds = [model(torch.tensor(img).unsqueeze(0).to(device))[0].reshape(5, 10).argmax(axis=-1).cpu().numpy().tolist() for img in imgs]\n",
    "    display(utils.merge_images(images, grid_size=(4, 1)))\n",
    "    print(labels)\n",
    "    print(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08a31d8",
   "metadata": {},
   "source": [
    "Discussion:\n",
    "- How reliable are accuracy metrics for evaluating the model? Would other metrics offer additional insights?\n",
    "    - Single accuracy is generally reliable overall but may mask specific class biases or sequence errors. It is recommended to check per-character accuracy, precision/recall, confusion matrix, sequence edit distance, or character-level F1-score simultaneously to comprehensively evaluate the model.\n",
    "- What kinds of errors are the most common—misrecognition of certain digits, complete failures? How could you address these types of errors?\n",
    "    - Common errors include confusion between numbers with similar shapes (such as 1/7 and 3/8), missed judgments of entire captchas, and swapping of preceding and following characters. These issues can be improved by increasing the proportion of such hard samples in the dataset, applying color/geometric augmentation, using attention mechanisms or CTC decoding, and introducing language model constraints, among other methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358e36a1",
   "metadata": {},
   "source": [
    "## Part 6: Discussion\n",
    "There are 11 discussion questions above. You need to answer all of them, each worth 1 point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199d79b4",
   "metadata": {},
   "source": [
    "## Bonus: Alphabet CAPTCHA Recognition\n",
    "\n",
    "In this part, you are encouraged to extend your model to recognize alphabetic CAPTCHAs (A–Z, a–z).\n",
    "We have provided some pre-generated alphabet CAPTCHA images for test.\n",
    "\n",
    "To complete this part, you should:\n",
    "- Show several examples of the training data you generated or used.\n",
    "- Report the modifications you made compared to the numeric CAPTCHA setting (e.g., data generation, model architecture, or training strategy).\n",
    "- Report your training accuracy.\n",
    "- Report your accuracy on the provided test set.\n",
    "\n",
    "You can add new cells below to organize your experiments, show results, and summarize your findings.\n",
    "Feel free to explore different ideas — creativity is encouraged!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00304d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "transform = None\n",
    "batch_size = 128\n",
    "folder = 'test_alphabet'\n",
    "from cs172 import utils\n",
    "from cs172 import generate_alpha\n",
    "need_rotate = True \n",
    "images = [generate_alpha.generate_verification_alpha(need_rotate=need_rotate)[0] for _ in range(16)]\n",
    "display(utils.merge_images(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0496a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------\n",
    "#\n",
    "# Data generation for alphabet recognition\n",
    "#\n",
    "#---------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "need_rotate = True # if you do not want to rotate the image for training, feel free to set it to False\n",
    "image_size = (260, 80)\n",
    "data_num = 100000 # total data num\n",
    "save_folder = 'data_alpha' # set data folder\n",
    "\n",
    "# You can comment out the line to avoid duplicate data generation\n",
    "generate_alpha.save_certification_data_alpha(image_size, data_num, save_folder, need_rotate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51992363",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from cs172.datasets_alpha import ImageDataset_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53859926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can try different transform for data augmentation\n",
    "# torchvision.transforms may be useful\n",
    "transform = None\n",
    "\n",
    "save_folder = 'data_alpha'\n",
    "\n",
    "\n",
    "from torchvision import transforms\n",
    "from cs172.datasets_alpha import RandomGrayscaleOrColor\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((227, 227)),\n",
    "    RandomGrayscaleOrColor(),\n",
    "    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.2),\n",
    "    transforms.RandomAffine(\n",
    "        degrees=25,\n",
    "        translate=(0.15, 0.15),\n",
    "        shear=10,\n",
    "        scale=(0.85, 1.15),\n",
    "        interpolation=transforms.InterpolationMode.BICUBIC\n",
    "    ),\n",
    "    transforms.RandomPerspective(distortion_scale=0.3, p=0.3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)\n",
    "])\n",
    "dataset = ImageDataset_alpha(imgdir=save_folder, transform=train_transform)\n",
    "print(len(dataset))\n",
    "\n",
    "ratio = 0.8\n",
    "train_size = int(ratio * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, valid_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# you can modify batch_size if 'CUDA OUT OF MEMORY'\n",
    "batch_size = 128\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size,\n",
    "    shuffle=True, num_workers=0)\n",
    "valid_dataloader = torch.utils.data.DataLoader(\n",
    "    valid_dataset, batch_size=batch_size,\n",
    "    shuffle=False, num_workers=0)\n",
    "\n",
    "img, label = next(iter(train_dataloader))\n",
    "print(img[0].min(), img[0].max())\n",
    "print(label[0])\n",
    "img.shape, label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e51fcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cs172.networks_alpha import get_model_alpha\n",
    "\n",
    "model_name = \"myresnet18\"\n",
    "assert model_name in (\"myresnet18\", \"resnet18\", \"resnet34\")\n",
    "model = get_model_alpha(model_name)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd65eb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "------------------------------------------------\n",
    "\n",
    "Test the networks or can be used as training demo\n",
    "\n",
    "------------------------------------------------\n",
    "\"\"\"\n",
    "from cs172.pipeline_alpha import train_alpha, test_alpha\n",
    "\n",
    "img, label = next(iter(train_dataloader))\n",
    "print(img.shape, label.shape)   \n",
    "\n",
    "\n",
    "from cs172.metrics_alpha import ImageAccuracy_alpha, DigitAccuracy_alpha\n",
    "from torchmetrics import MetricCollection\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "metrics = MetricCollection({\n",
    "    \"image_acc\": ImageAccuracy_alpha(),\n",
    "    \"digit_acc\": DigitAccuracy_alpha(),\n",
    "}).to(device)\n",
    "\n",
    "img, label = next(iter(train_dataloader))\n",
    "img, label = img.to(device), label.to(device)\n",
    "targets = label.argmax(dim=-1).long()   # [B,5]\n",
    "\n",
    "for step in range(500):\n",
    "    optimizer.zero_grad()\n",
    "    pred = model(img).view(-1, 5, 52)\n",
    "    loss = F.cross_entropy(pred.reshape(-1, 52), targets.reshape(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % 50 == 0:\n",
    "        metrics.reset()\n",
    "        metrics.update(pred, label)\n",
    "        accs = metrics.compute()\n",
    "        print(f\"Step {step:03d} | Loss={loss.item():.4f} | \"\n",
    "              f\"ImageAcc={accs['image_acc']:.3f} | DigitAcc={accs['digit_acc']:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a412938",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "if os.path.exists(\"model_weights_alpha.pth\"):\n",
    "    model_weights = torch.load(\"model_weights_alpha.pth\")\n",
    "    model.load_state_dict(model_weights)\n",
    "else:\n",
    "    # you can try different hyperparameters by passing args `lr`, `weight_decay`, `num_epoch`\n",
    "    train_alpha(model, device, train_dataloader,lr=1e-4)\n",
    "\n",
    "# save model weights, thus you can try different hyperparameters without retraining\n",
    "torch.save(model.state_dict(), \"model_weights_alpha.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f7d7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else 'cpu'\n",
    "metrics = test_alpha(model, device, valid_dataloader)\n",
    "for key, value in metrics.items():\n",
    "    print(f\"{key}:\", value.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a999eb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = ImageDataset_alpha(os.path.join('samples', folder), transform=transform)\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=batch_size,\n",
    "    shuffle=False, num_workers=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbf3e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = test_alpha(model, device, test_dataloader)\n",
    "print(\"\\n Test Results:\")\n",
    "for key, value in results.items():\n",
    "    print(f\"{key}: {value.item():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "172a1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
